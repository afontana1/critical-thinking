---
title: "bending truth how adults get indoctrinated"
---
<br>

<div>
<p>
People who make it out of groups like exploitative religious and pseudoscientific organisations are often faced with distortions from two directions. When they attempt to convey their experiences of manipulation to members of their former group their words are magically transformed into 'I'm mad, evil and weak'. But when they share their story  with non-members as a cautionary tale their words are often magically transformed into 'I'm gullible, asinine and spineless'. 
</p>
</div>
<br>

<div>
<p>
After a while, the apostates realise they're in the noonesland of the unheard. And it begins to become clear why these groups continue to thrive — groups with a talent for bending truth, preaching love while denouncing non-members as a subhuman underclass; claiming to champion peace, while committing acts of barbarism; pontificating about family values while tearing up countless homes; promising freedom and enlightenment while enforcing ignorance and exercising total control I want to explore some of the mechanics of manipulative groups, 
</p>
</div>
<br>

<div>
<p>
dispatching some of the stereotypes about the people who get recruited by them, and reflecting on preventive measures we might take to avoid recruitment. But if there's just one thing I really want to do in this video, it's to communicate the importance of acknowledging a simple but profound fact: we can be manipulated At a press conference on manipulative groups Ian reveals he lost tens of thousands of dollars When he was recruited by a group who proposed that clusters of alien spirits are attached to all humans, keeping us from a full potential. 
</p>
</div>
<br>

<div>
<p>
Ian tells us how he and the other group members paid through the nose — and every other orifice — for a special kind of counselling which they were told would rid them of these spirits. They were told that once these spirits were gone, they would be the primary causal agents of their lives with skills such as telekinesis, total recall and mastery over illness. No evidence was given for any of this. 
</p>
</div>
<br>

<div>
<p>
The alien story was the unsubstantiated revelation claim of the group's founder and no records have ever been produced of a single person gaining any of the special powers or knowledge described. What do we think of Ian? Does he lack intelligence? No. His intelligence is above average. He's a highly creative, resourceful man —&nbsp;that's why the group considered him such a juicy prospect. Is he a bit credulous — quick to accept whatever he's told? No. In fact, by default, he's a cautious skeptical fellow. 
</p>
</div>
<br>

<div>
<p>
Is he driven by narcissism, desiring supremacy over others? No. He's always been compelled by deeply humanitarian instincts. Of course there are folks out there who are .... gullible, obtuse, submissive, power-hungry, escapist, lazy. But it's a huge mistake to assume that successful recruitment into manipulative groups depends on the presence of these kinds of traits. And while we're hunting for them in Ian's case, we're missing six crucial words: 'he was recruited by a group'. 
</p>
</div>
<br>

<div>
<p>
He was subjected to a process. The idea of alien spirit clusters wasn't presented to him in the bald way he presented it to us. Ian was carefully steered through a sequence of experiences so that by the time the alien spirits concept was mentioned a range of key psychological elements had been established to ease its entry and help insulate it from criticism. Later on, I'll be looking at how this was done. 
</p>
</div>
<br>

<div>
<p>
Just as we can become distracted by the red herring of personality deficiencies we can become misled by the notion of predisposing circumstance. Some psychologists have generated lists of situational factors that can leave us vulnerable to manipulative groups. Factors like 'weak family bonds' or 'recent bereavement'. Problem is, when we don't find ourselves reflected in those items, we can be left with the impression we're not vulnerable. 
</p>
</div>
<br>

<div>
<p>
When we see an item like 'weak family bonds' we might satisfy ourselves that we not risk because we have strong family bonds. But strong family bonds can put people at risk too. Many people are recruited by relative. And if your whole family's been indoctrinated, you could be born into a manipulative group. In these cases, even if you wake up and smell the Kool Aid strong family bonds might keep you feeling trapped inside the group under the threat of disconnection from your family. 
</p>
</div>
<br>

<div>
<p>
Members of manipulative groups are a diverse population and it serves us well to think creatively about how these groups might hook us in. Some of us go through life convinced we possess an almost clinical objectivity — that, barring deliberate deceptions or the odd quirky optical illusion, we generally perceived reality as it actually is; and that we make decisions on a firmly rational basis. For over a century, social psychologists  have been ripping that fantasy apart, conducting experiments that elicit the most bizarre, counterintuitive human responses 
</p>
</div>
<br>

<div>
<p>
and exposing a gamut of distinct biases each of which distorts our perception in its own specific way. A landmark concept in social psychology  emerged in the 1950s that provided a framework on which to untangle all kinds of paradoxical human behaviour. Leon Festinger's theory of cognitive dissonance. It also described arguably the biggest implement in the toolboxes of manipulative groups who not only rely on it to keep existing members under their influence but also actively exploit it to recruit new members. 
</p>
</div>
<br>

<div>
<p>
Festinger and his colleagues proposed that  we seek consistency within our cognitions — our thoughts, emotions, memories, perceptions — and that when we discover an inconsistency we experience a psychological discomfort that motivates us to try and reduce that conflict. So, what does dissonance look like in action? Say we've been told to believe all members of group A evil. One day, a friend we've always known to be a good person reveals she's a member of group A. 
</p>
</div>
<br>

<div>
<p>
This arouses dissonance because our experience contradicts our belief. We can reduce this dissonance in a number of ways. If we're not hugely invested in the belief, we might opt for method one: simply adjust our belief. We might, for instance, change it to: 'not all members of group A are evil'. If we're equally invested in the belief and the friendship we might try to preserve both using method two: denial. 
</p>
</div>
<br>

<div>
<p>
We might refuse to believe our friend is a true member of group A, granting her special exemption because she's just not like those other folks. If we're more invested in the belief than the friendship, we might go for method three: select confirming information. Here, we cherry-pick information consistent with our belief and ignore information that contradicts it. Any flaws in our friend's behaviour or attitudes can be turned against her while her goodness is ignored. 
</p>
</div>
<br>

<div>
<p>
Festinger and his colleagues noted that when we experience dissonance we'll tend to avoid information or situations that threaten further dissonance. For instance, if we've denied that our friend is a member of group A, we'll want to avoid the subject in future conversations. Or, if we've used selective information to cast our friend as evil, we might quickly break off contact to avoid giving her the chance to reason with us. 
</p>
</div>
<br>

<div>
<p>
But sometimes our response to dissonance is so horrendously distorted that it makes us almost invulnerable to further dissonance, reducing our need for avoidance. For example, say we look for evidence that our friend's evil, but can't find any? What if — time after time — we're confronted with good? How can we possibly preserve a belief that all members of group A are evil now? Well, all is not as bleak as it seems. 
</p>
</div>
<br>

<div>
<p>
We have method four: repackaging. Believing that no member of group A could possibly be good We could repackage our experience  of our friend's goodness as something it's not. We might tell ourselves she was only pretending to be good — it was all an act, a deception. And isn't that the most insidious evil of all? The evil that disguises itself as good? The wolf in sheep's clothing? We've now put our friend in a no-win position. 
</p>
</div>
<br>

<div>
<p>
Even her goodness has now become evidence of her evil. Now we don't even have to avoid her. Anything she does — good or bad — confirms our belief. Later on, I'll be looking at how manipulative groups explicitly employ these kinds of extreme contortions to put their critics in a no-win position and keep their members under psychological lockdown. Various experiments have shown how dissonance can be actively exploited to induce positive responses. 
</p>
</div>
<br>

<div>
<p>
In a 1959 paper Aronson and Mills investigated the role of initiation procedures in attitudes toward groups. They advertised for female subjects to join a group discussing the psychology of sex. But before being accepted, subjects were told they had to pass an embarrassment test. There were three experimental conditions. Subjects in the 'severe initiation' condition read out loud to the experimenter a list of words considered obscene as well as sexually explicit passages. 
</p>
</div>
<br>

<div>
<p>
Subjects in the 'mild initiation' condition read out loud a list of sexual but non-obscene terms. Subjects in the 'no initiation' condition weren't screened. Each subject then listened to the same pre-recorded discussion,&nbsp;deliberately designed to be boring. They then rated their perceptions of the group. Intuitively, one might think the more severe the initiation the more the discussion's dreariness would be accentuated. 
</p>
</div>
<br>

<div>
<p>
But the reverse was true. Subjects from the 'severe' condition showed the strongest liking for the group, rating it as significantly more attractive compared with the ratings of the other groups. Aronson and Mills explained this counter-intuitive finding in terms of dissonance between two cognitions: subjects in the 'severe' condition had gone through a difficult experience to join the group, but the group wasn't worthwhile. 
</p>
</div>
<br>

<div>
<p>
When we invest time, effort, money, emotion, in an enterprise we want to believe our investment isn't wasted. So we repackage useless enterprises as useful. This result, supported by other studies, demonstrates how worthless groups can be made to appear worthwhile. A decade later, Knox and Inkster showed just how quickly actions can induce attitudes. At a racetrack, 69 bettors were asked by one experimenter, just prior to betting to rate the chances of their horse winning. 
</p>
</div>
<br>

<div>
<p>
72 bettors were asked by a second experimenter, just after placing their bets. Using a 7-point scale, on average, pre-bettors rated their chances 'fair'. Post-bettors rated their chances 'good'. One bettor, who'd given a rating to the pre-bet experimenter and placed his bet then spotted the post-bet experimenter and asked if the two were working together. He then enquired if he could now change his rating from 'fair' to 'excellent'. 
</p>
</div>
<br>

<div>
<p>
Studies like this show the important role behaviour plays in dissonance. In fact, when we're looking at the components of dissonance we could divide them into three broad classes: behaviour, emotions and thoughts. Pull hard enough on one and, in our drive for consistency, the others will follow When we behave as if we're committed, our thoughts and emotions often fall in line. But how do we become so heavily invested in an idea that we find ourselves twisting concepts into their opposites — bending 'good' into 'evil', 'love' into 'hate' — just to preserve that idea? 
</p>
</div>
<br>

<div>
<p>
While cognitive dissonance studies tend to focus on the conflict between two ideas those two ideas don't of course exist in isolation. Networks of reinforcing ideas often lie behind them. For instance, regarding our hypothetical belief that group A is evil maybe our entire community has always vehemently condemned group A. We might have behaved hatefully toward some members of group A ourselves in the past based solely on this prejudice. 
</p>
</div>
<br>

<div>
<p>
We might also have seen group A sympathisers being attacked and ostracised. If it turns out group A isn't evil, we have some tough realities to face. First, we have to admit our entire community is wrong. This could tap into some hefty issues around trust, loyalty and credibility. Second, we'll have to admit our own hateful behaviour was unjustified. This could have big implications for our self-image. 
</p>
</div>
<br>

<div>
<p>
Third, we now face being attacked and ostracised if we defend groups A which taps into our ideas of safety and security. These tough realities might feel intensely threatening to us. But the threat goes away if we can find some way of preserving our belief that group A is evil. So it is that we might find ourselves turning on our friend just to preserve a false belief. And of course this might in itself make it even harder to admit our mistake in the future because adding to all the other tough realities 
</p>
</div>
<br>

<div>
<p>
we'll now have to admit we betrayed a good friend. Our minds are not objective, logical machines. Our minds are vast networks of ideas, striving not necessarily for truth but for internal consistency. In view of this, it's plain to see why childhood is by far the most preferred period for indoctrination into manipulative groups. Parents and guardians can get in there right at the start and insert any unsubstantiated concept they like into the child's basic initial framework without resistance. 
</p>
</div>
<br>

<div>
<p>
Being implanted so early they stand a good chance of becoming core beliefs with rich networks of ideas reinforcing them including strong associations of identity of many levels — individual, family, culture, perhaps nationality. If the unsubstantiated concept is later challenged, threatening tremors could be felt in all of these areas. Indoctrinating adults who've already developed an advanced cognitive network isn't so straightforward. 
</p>
</div>
<br>

<div>
<p>
Manipulative groups often have to insert their unsubstantiated ideas into a distinctly inhospitable environment. And it's no coincidence that in order to achieve that goal some groups will try to mimic those childhood conditions by infantilising adults. When I was a christian we were repeatedly told that we needed to be like children — trusting and dependent on Yahweh which in the total absence of any evidence of Yahweh translates to: 'trusting and dependent on the humans who claim to represent Yahweh'. 
</p>
</div>
<br>

<div>
<p>
And there was a nice little threat thrown in there. If you didn't become like children, you'd never get into heaven. In a similar vein we were told: 'Lean not on your own understanding'. 'Do not be wise in your own eyes'. The message in all these commands couldn't be clearer. Wherever your ideas conflict with those of the group, it's your ideas that need to change. Folks who submit to these commands can now have vast amounts of their cognitive network rewritten without substantial resistance — just like childhood. 
</p>
</div>
<br>

<div>
<p>
Of course, adult recruits are more of a bonus than a necessity for the major world religions who are guaranteed a steady influx of child recruits As long as the abuse of childhood religious indoctrination continues. Other groups who are more reliant on adult recruits often have to manipulate cognitive dissonance in more subtle ways. Let's go back to Ian and see how he gets recruited by a group claiming clusters of alien spirits are attached to us all keeping us from a full potential. 
</p>
</div>
<br>

<div>
<p>
Ian is approached on the street by a woman with a personality questionnaire. He agrees to accompany her inside the official premises behind her where he spends half an hour completing it. His results show him to be unstable, depressed, nervous and overly critical. These conflict with his self-image. But the woman talks with him for a further half hour about his life — his frustrations, his disappointments — and draws tentative connections with his results. 
</p>
</div>
<br>

<div>
<p>
She says her group runs courses that can help him become a more effective person. Ian's natural caution stops him from signing up for a course but he agrees to buy a cheap book. He now experiences his first cognitive dissonance in the group's favour. He has two conflicting cognitions: one, that he's not interested; but two, that he's just invested an hour of his time and bought a book. Dissonance leads him to adjust his first cognition and rationalise that he is interested. 
</p>
</div>
<br>

<div>
<p>
Going home, he dips into the book and reflects on the idea of becoming a more effective person. The questionnaire results begin to play on his mind. Despite himself, he begins to select confirming information finding memories that match the results. Deciding he has nothing to lose by giving it a go, he returns and signs up for a course on communication. Ian is now locked into a commitment cycle. The more time and money he invests, the more he becomes convinced it's worthwhile The easier he's persuaded to invest more time and money in further courses. 
</p>
</div>
<br>

<div>
<p>
The courses include odd activities like sitting in silence, staring at another member or endlessly picking up the same object. These meaningless tasks confuse Ian And induce trance states and hallucinations increasing his susceptibility to direction. At the end of each course Ian has to write up and read out loud to the group how the course helped him. This forces him not only to find meaning in the meaningless tasks but to declare it publicly. 
</p>
</div>
<br>

<div>
<p>
Declaring benefits he doesn't really feel creates dissonance which Ian reduces by rationalising that he does feel benefits. His dissonance is further reduced in two ways. First, Ian's speech receives huge praise and applause from other members validating his words and building on his feelings of acceptance and loyalty to the group. Second, the testimony of other members further justifies the idea that the tasks have meaning. 
</p>
</div>
<br>

<div>
<p>
Other activities train Ian not to react to provocation. For instance, when being goaded with insults and jokes by other members. Ian learns to disconnect from confrontation. He rationalises that this training is helping him focus. It'll soon be put to other uses. Ian's told of the group's interest in the betterment of humankind. Their aim is 'total freedom for everyone on the planet'. This resonates powerfully with his own humanitarian concerns. 
</p>
</div>
<br>

<div>
<p>
But he's told about critics who want to sabotage this important work, spreading lies and alienating members. The group encourages Ian to avoid mainstream media for this reason. He's always been against censorship. But censorship has now been repackaged as a sensible precaution against destructive critics. Thee group now controls all information about the group. When later confronted by critics in person Ian automatically disconnects from them, as he's been trained to do blocking out the very folks who could help him. 
</p>
</div>
<br>

<div>
<p>
In everyday life, Ian feels better, sharper, more confident more effective. But it's an illusion. With the group's courses pitched at kindergarten levels, in developmental terms, far from being stretched Ian's been slowly regressing. After completing the basic courses Ian's processed straight into the next level. A strange over-ritualised kind of counseling is introduced using lie-detector like technology to gauge his emotional reaction as he repeatedly confronts difficult memories. 
</p>
</div>
<br>

<div>
<p>
The aim is to nullify all emotional reaction to past traumas until it no longer registers on the apparatus. These counselling sessions are expensive. Sometimes very tiring. But he frequently emerges feeling lighter. The sessions take him further and further into the past until he's not quite sure if he's dealing with real memories. Ian visits his parents. He talks about the group. His father is concerned about the money Ian's getting through and does an internet search on the group. 
</p>
</div>
<br>

<div>
<p>
He's disturbed at what he reads and asked Ian to take a look but Ian declines, saying it's all lies. When he tells the group about his parents' reaction. Ian finds himself being interviewed at length with concerns expressed about how their influence could seriously damage his progress. At the end of an exhausting session Ian agrees to only discuss superficial subjects with his parents. He takes the issue to counselling to nullify his emotional reaction. 
</p>
</div>
<br>

<div>
<p>
The groups' noble aim of total freedom for everyone on the planet has been repackaged. 'Total freedom' now means: 'freedom to do what the group allows'. 'Everyone on the planet' now means 'everyone who remains uncritical of the group'. Three years and thousands of dollars later Ian's completed two levels of counseling. He's also expecting his second child with his new wife, also a member of the group. 
</p>
</div>
<br>

<div>
<p>
Time for level three. He's led into a room and shown a document that tells him the following information: Millions of years ago, an evil galactic ruler transported billions of aliens to earth in spaceplanes placed them around volcanoes and blew them up with nuclear weapons. As they departed from their bodies, the alien spirits were then captured in electronic traps and subjected to a month-long 3d propaganda movie that implanted false ideas in them including the world religions. 
</p>
</div>
<br>

<div>
<p>
These alien spirits roamed the earth aimlessly and when humans evolved they attached themselves in clusters to their bodies. These alien spirits are what keep humans from fulfilling their full potential. Ian now experiences perhaps the most violent dissonance in his life. If he rejects the alien story, he's out. And all the time, energy and money he's invested in the group has been wasted. The group's strict policy of shunning dissenters also means the loss of his wife, his children and all his friends. 
</p>
</div>
<br>

<div>
<p>
But beyond even these threatening realities is a deep-seated belief that will keep him under the group's influence haemorrhaging money for another two years: 'I could never be manipulated'. To him, the only folks who fall for cons are gullible, acquiescent, stupid. And that's just not him. And indeed that wasn't him Nonetheless he was recruited. Not because of some personality deficiency. But simply because he's a human being who — like all of us — possesses a brain that's evolved to strive towards consistency. 
</p>
</div>
<br>

<div>
<p>
In his extreme state of dissonance Ian is led directly into a counselling session to nullify his emotional reaction. A few years ago, a woman wrote to me to tell me she'd once thought I used demonic psychological techniques in my videos. She'd been in a very controlling fundamentalist religious group at the time and was disturbed to find herself feeling persuaded by some of the atheist counter-arguments I was presenting. 
</p>
</div>
<br>

<div>
<p>
She was convinced I'd used satanic mind control to put her under a spell. This was no random rationalisation. Her group had told her this was how the atheists got you. And she was terrified it had happened to her. To try and break the spell she watched the same videos over and over to figure out exactly how I'd corrupted her. It was during this process of analysis that she came to realise the simpler explanation: she'd been persuaded by legitimate counter-arguments. 
</p>
</div>
<br>

<div>
<p>
She then realised something else: she'd become an atheist. This woman's story is an illustration of how manipulative groups develop strategies to put critics in no-win situations. If I fail to persuade, I lose. If I succeed in persuading, I must be cheating, therefore I lose. Manipulative groups often possess a wealth of ready-made rationalisations deviously designed so that their members' cognitive dissonance is resolved in the group's favour. 
</p>
</div>
<br>

<div>
<p>
Some of them involve splitting a single concept into two categories. We saw this with our hypothetical friend from evil group A whose goodness was invalidated by repackaging it as a deception. Whatever good she did only proved her deceitfulness. Here, goodness was effectively split into true goodness and false goodness. Group A's goodness was always false. The authors of the Bible perform a similar split in regard to wisdom dividing it into true godly wisdom and false worldly wisdom. 
</p>
</div>
<br>

<div>
<p>
Worldly wisdom, which employs human philosophy reason and logic, is dismissed in various books including Proverbs and Romans. But 1 Corinthians goes further. In chapter 1, we're told that godly wisdom appears foolish to the worldly wise; that Yahweh has deliberately chosen foolish means by which to confound the worldly wise turning the worldly wise into fools. This repackaging of wisdom allows the group to rebuff all criticisms out of hand. 
</p>
</div>
<br>

<div>
<p>
Of course the critics think their religion is foolish! Their God deliberately made their beliefs incomprehensible to the worldly wise! Manipulative groups sometimes try to shield themselves from their critics by keeping their beliefs secret. Some groups suggest their beliefs are so potent they could strike down or even kill the uninitiated. When he was recruited into the Unification Church, otherwise known as the Moonies, Steven Hassan, a leading writer, speaker and counsellor in the field of manipulative groups. 
</p>
</div>
<br>

<div>
<p>
was told: 'You wouldn't feed a baby thick pieces of steak would you? .... .... You have to feed a baby something it can digest, like formula .... .... Well these people are spiritual babies. Don't tell them more than they can handle .... .... or they'll die.' Lafayette Ronald Hubbard, founder of pseudoscientific dianetics and scientology suggested that if folks were told about Scientology's Xenu (Xemu) creation myth without appropriate preparation, they might develop pneumonia. 
</p>
</div>
<br>

<div>
<p>
The Xenu story has become pretty common knowledge due to countless testimonies from ex-scientologists. But a global Xenu-induced pneumonia epidemic has yet to be reported. Some groups claim it's pointless to try and explain beliefs that are too advanced for non-members to comprehend. Heaven's Gate was a UFO religious group who committed mass suicide by ingesting phenobarbital mixed into apple sauce believing this would allow their souls to be picked up on a spacecraft trailing the Hale-Bopp comet. 
</p>
</div>
<br>

<div>
<p>
In a testimony video recorded shortly before their deaths one member used the analogy of a person on a doctoral degree in quantum mechanics trying to explain to a second grader what he's learned. Another favourite manoeuvre used by manipulative groups to deflect criticism is to develop a persecution narrative. Here, rather than splitting a single concept into two, the group will instead join distinct concepts into one. 
</p>
</div>
<br>

<div>
<p>
Persecution is about the systemic mistreatment of an individual or group. It's about cruel and unfair behaviour. Where a group is angrily criticised and condemned for social legal and/or ethical violations that's not persecution. That's protest. Government officials jailed for criminally denying the rights of people they personally deem unworthy are not being persecuted for their religious beliefs. 
</p>
</div>
<br>

<div>
<p>
Parents jailed for letting children die by denying them vital medical assistance. are not being persecuted for their religious beliefs. They're being punished for their own cruel and unfair behaviour towards their fellow humans. By repackaging appropriate protest and punishment as persecution not only do manipulative groups get to commit all kinds of abuses guilt-free. they get to feel like martyrs while they're doing it. 
</p>
</div>
<br>

<div>
<p>
Consoling them further are passages from their holy books that promise great rewards for being persecuted — after they die of course. It's been put to me more than once that critics don't stand a chance against belief systems that are so psychologically ring-fenced and can wield such gargantuan carrots and sticks. And yet critics to get through. Though the odds are stacked in the group's favour dissonance isn't guaranteed to go their way. 
</p>
</div>
<br>

<div>
<p>
Time after time, something sticks. A phrase. An image. An argument. A question. That tiny fragment of clarity is sometimes all it takes to instigate a process of self-extraction. So critics take heart — and speak out. Our minds are vast networks of ideas, striving not necessarily for truth but for internal consistency so that that the injection of a false idea into that network can have a domino effect distorting the ideas around it. 
</p>
</div>
<br>

<div>
<p>
From this, three things become clear. One, the importance of anchoring our beliefs in firm evidence and valid, sound logic. The more we lower these standards, the easier false ideas slip inside. Two, the importance of getting comfortable with not knowing. If we don't know whether an assertion is true or false we need to be real about that. Taking the dishonest position of 'I know' based on what we want to believe could lead to false rationalisations if future evidence contradicts us. 
</p>
</div>
<br>

<div>
<p>
Taking be honest position of 'I don't know' mean future evidence can be assessed openly Three, the importance of understanding the techniques of manipulative groups. These groups are practiced at insinuating false ideas into our cognitive networks gradually turning our worlds upside down so that while we think we're still grounded in reality we've instead hooked ourselves to clouds. and we're drifting slowly away from all sense and reason. 
</p>
</div>
<br>

<div>
<p>
One of the best preventative measures against recruitment? Learn about recruitment techniques. One of the best ways of doing that? Listen to the stories of the people who've been there. Manipulative groups are often made of folks just like you and me. That's what makes them so effective — because the folks who recruit us are just like us. Another preventive measure is to disarm the beliefs that can be used against us to induce dissonance. 
</p>
</div>
<br>

<div>
<p>
As Aronson and Mills showed we're sometimes fooled into thinking that groups who get us to jump through hoops are worthwhile because we want to believe our time isn't wasted. That phrase 'want to believe' is like a signpost, saying: 'Examine me!' Is this belief justified or is it a wishful distortion? The fact is our time is sometimes wasted by undeserving pursuits. When we acknowledge that, this particular manipulation is disarmed. 
</p>
</div>
<br>

<div>
<p>
And we can apply the same principle to many other ideas we want to believe that can be used against us. For instance, that we're always right or that we can always judge a person's character. I'm sure you can think of other examples. I'll end on one: wanting to believe we can't be manipulated. Of course we're manipulated all the time. Manipulated by adverts to buy products. Manipulated by loved ones to do favours. 
</p>
</div>
<br>

<div>
<p>
But if we ever have the misfortune of getting recruited by a manipulative group this wishful distortion could block all our escape routes, keeping us in a state of denial. Seems paradoxical, but the moment we admit we can be manipulated We instantly reduce our chances of being manipulated because we become more open to evidence that we have been. Chances are some folks watching this are in a manipulative group right now excusing their group's abuses and hypocrisies. 
</p>
</div>
<br>

<div>
<p>
The more aware we are of their methods and of our own human quirks and biases the less successful they'll be in keeping us doing what keeps them alive: bending truth. 
</p>
</div>
<br>